{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If haven't already downloaded all NLTK\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cosine similarity function\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Data storage and manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If running large dataset, used for indicating time through loops\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y, stop_words):\n",
    "    \"\"\"\n",
    "    Takes two sentences (string), x and y, and returns cosin similarity value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove special characters\n",
    "    x = re.sub(\"[^a-zA-Z]\", \" \", x)\n",
    "    y = re.sub(\"[^a-zA-Z]\", \" \", y)\n",
    "    \n",
    "    # Tokenize sentences / transform into list of words\n",
    "    x_list = word_tokenize(x)\n",
    "    y_list = word_tokenize(y)\n",
    "    \n",
    "    # Initialize lists that will be used for vectors\n",
    "    x_vector = []\n",
    "    y_vector = []\n",
    "    \n",
    "    # Remove stop words from the tokenized lists and get union set\n",
    "    x_set = {word for word in x_list if not word in stop_words}\n",
    "    y_set = {word for word in y_list if not word in stop_words}\n",
    "    union_set = x_set.union(y_set)\n",
    "    \n",
    "    # Create vectors\n",
    "    for word in union_set:\n",
    "        if word in x_set:\n",
    "            x_vector.append(1)\n",
    "        else:\n",
    "            x_vector.append(0)\n",
    "        if word in y_set:\n",
    "            y_vector.append(1)\n",
    "        else:\n",
    "            y_vector.append(0)\n",
    "    \n",
    "    # Cosine calculation\n",
    "    c = 0\n",
    "    for i in range(len(union_set)):\n",
    "        c += x_vector[i] * y_vector[i]\n",
    "    cosine_similarity = c / float((sum(x_vector) * sum(y_vector))**0.5)\n",
    "    \n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "def get_closest_clusters(cluster_dict, threshold, stop_words):\n",
    "    \"\"\"\n",
    "    Takes cluster dictionary where keys are clusters and values are utterances\n",
    "    Returns list of two keys representing closest clusters\n",
    "    \"\"\"\n",
    "    # start = time.time()\n",
    "    \n",
    "    max_similarity = 0\n",
    "    combine_clusters = []\n",
    "\n",
    "\n",
    "    # Iterate through all combinations of clusters using key index so I don't repeat combinations\n",
    "    for cluster_i in range(len(cluster_dict.keys())):\n",
    "        for comparison_cluster_i in range(cluster_i+1, len(cluster_dict.keys())):\n",
    "\n",
    "            # Get actual keys / cluster numbers\n",
    "            key_1 = list(cluster_dict.keys())[cluster_i]\n",
    "            key_2 = list(cluster_dict.keys())[comparison_cluster_i]\n",
    "            \n",
    "\n",
    "            # If not the same cluster\n",
    "            if key_1 != key_2:\n",
    "\n",
    "                # Check similarity of the utteranceclusters using average if the utterance\n",
    "                # is already clustered.Retain keys of clusters or utterances with max similarity\n",
    "\n",
    "                # If comparing cluster to utterance\n",
    "                if (len(cluster_dict[key_1]) > 1) and (len(cluster_dict[key_2]) == 1):\n",
    "                    sum_sim = 0\n",
    "                    for utterance in cluster_dict[key_1]:\n",
    "                        sum_sim += cosine_similarity(utterance, cluster_dict[key_2][0], stop_words)\n",
    "                    similarity = sum_sim / len(cluster_dict[key_1])\n",
    "                    if similarity > max_similarity:\n",
    "                        max_similarity = similarity\n",
    "                        combine_clusters = [key_1, key_2]\n",
    "\n",
    "                # If comparing utterance to cluster\n",
    "                elif (len(cluster_dict[key_1]) == 1) and (len(cluster_dict[key_2]) > 1):\n",
    "                    sum_sim = 0\n",
    "                    for utterance in cluster_dict[key_2]:\n",
    "                        sum_sim += cosine_similarity(cluster_dict[key_1][0], utterance, stop_words)\n",
    "                    similarity = sum_sim / len(cluster_dict[key_2])\n",
    "                    if similarity > max_similarity:\n",
    "                        max_similarity = similarity\n",
    "                        combine_clusters = [key_1, key_2]\n",
    "\n",
    "                # If comparing cluster to cluster\n",
    "                elif (len(cluster_dict[key_1]) > 1) and (len(cluster_dict[key_2]) > 1):\n",
    "                    sum_sim = 0\n",
    "                    for utterance_1 in cluster_dict[key_1]:\n",
    "                        for utterance_2 in cluster_dict[key_2]:\n",
    "                            sum_sim += cosine_similarity(utterance_1, utterance_2, stop_words)\n",
    "                    similarity = sum_sim / (len(cluster_dict[key_1]) * len(cluster_dict[key_2]))\n",
    "                    if similarity > max_similarity:\n",
    "                        max_similarity = similarity\n",
    "                        combine_clusters = [key_1, key_2]                \n",
    "\n",
    "                # If comparing utterance to utterance\n",
    "                else:\n",
    "                    similarity = cosine_similarity(cluster_dict[key_1][0], cluster_dict[key_2][0], stop_words)\n",
    "                    if similarity > max_similarity:\n",
    "                        max_similarity = similarity\n",
    "                        combine_clusters = [key_1, key_2]\n",
    "    \n",
    "    # Checks to make sure the similarity of the clusters is above the threshold\n",
    "    if max_similarity < threshold:\n",
    "        combine_clusters = []\n",
    "\n",
    "    # end = time.time()\n",
    "    # print('get_closest_clusters: {}'.format(end-start)) \n",
    "        \n",
    "    return combine_clusters\n",
    "\n",
    "def update_cluster_dict(combine_clusters, cluster_dict):\n",
    "    \"\"\"\n",
    "    Takes combine_clusters list of two keys representing clusters to be combined and the cluster dictionary\n",
    "    Adds all values from second key to first key values, deletes second key\n",
    "    Returns updated cluster dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # start = time.time()\n",
    "    \n",
    "    for key, value in cluster_dict.items():\n",
    "        if key == combine_clusters[1]:\n",
    "            cluster_dict[combine_clusters[0]].append(cluster_dict[combine_clusters[1]][0])\n",
    "    del cluster_dict[combine_clusters[1]]    \n",
    "\n",
    "    # end = time.time()\n",
    "    # print('update_cluster_dict: {}'.format(end-start))    \n",
    "\n",
    "    return cluster_dict\n",
    "\n",
    "def get_wcas(cluster_dict, stop_words):\n",
    "    \"\"\"\n",
    "    Takes cluster dictionary\n",
    "    Returns within cluster average similarity\n",
    "    \"\"\"\n",
    "    # start = time.time()\n",
    "    \n",
    "    wcas = {}\n",
    "    for key, value in cluster_dict.items():\n",
    "        if len(value) > 1:\n",
    "            sum_sim = 0\n",
    "            d = 0\n",
    "            for i in range(len(value)):\n",
    "                for j in range(i+1, len(value)):\n",
    "                    sum_sim += cosine_similarity(value[i], value[j], stop_words)\n",
    "                    d += 1\n",
    "            wcas[key] = sum_sim / d\n",
    "    \n",
    "    sum_sim = 0\n",
    "    for value in wcas.values():\n",
    "        sum_sim += value\n",
    "    overall_wcas = sum_sim / len(wcas.values())\n",
    "    \n",
    "    # end = time.time()\n",
    "    # print('get_wcas: {}'.format(end-start))\n",
    "    \n",
    "    return overall_wcas, wcas\n",
    "\n",
    "def utterance_hierarchical_clustering(cluster_dict, threshold, stop_words):\n",
    "    \"\"\"\n",
    "    Takes cluster dictionary and threshold value between 0 and 1 for the lowest cosine similarity\n",
    "    within any cluster\n",
    "    Returns cluster dictionary, overall within cluster average similarity list, final overall within cluster\n",
    "    average similarity, and within cluster average similarity dictionary\n",
    "    \"\"\"\n",
    "    # start_time = time.time()\n",
    "    \n",
    "    overall_wcas_list = []\n",
    "    overall_wcas = 0\n",
    "    wcas = {}\n",
    "    num_clusters = len(cluster_dict.keys())\n",
    "    lowest_similarity = 1\n",
    "\n",
    "    # Clustering loop runs until there is only 1 cluster, the within cluster average similarity for one of\n",
    "    # the clusters dips below threshold, or there is 0 similarity between any cluster\n",
    "    while (num_clusters > 1) and (lowest_similarity > threshold):\n",
    "               \n",
    "        combine_clusters = get_closest_clusters(cluster_dict, threshold, stop_words)\n",
    "\n",
    "        if len(combine_clusters) == 0:\n",
    "            break\n",
    "\n",
    "        cluster_dict = update_cluster_dict(combine_clusters, cluster_dict)                    \n",
    "        overall_wcas, wcas = get_wcas(cluster_dict, stop_words)\n",
    "        num_clusters = len(cluster_dict.keys())\n",
    "        overall_wcas_list.append(overall_wcas)\n",
    "        \n",
    "        # execution_time = time.time() - start_time\n",
    "        # start_time = time.time()\n",
    "        \n",
    "        # print(execution_time, num_clusters)\n",
    "        \n",
    "    return cluster_dict, overall_wcas_list, overall_wcas, wcas\n",
    "\n",
    "def print_results(cluster_dict, overall_wcas, wcas):\n",
    "    print('Final Overall WCAS: {} \\n'.format(overall_wcas))\n",
    "    print('Cluster \\t WCAS \\t \\t \\t \\t Length')\n",
    "    for k, v in wcas.items():\n",
    "        print('{} \\t \\t {} \\t \\t {}'.format(k, v, len(cluster_dict[k])))\n",
    "    print('\\n')\n",
    "    print('Cluster \\t Utterance')\n",
    "    for k, v in cluster_dict.items():\n",
    "        print('{} \\t \\t {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read utterances into Pandas df\n",
    "utterances = pd.read_csv('Validation Data.csv', header=0, names=['Utterance'])\n",
    "\n",
    "# Put utterances into cluster dictionary where each utterance is it's own cluster\n",
    "cluster_dict = {}\n",
    "for i in range(len(utterances.values.tolist())):\n",
    "    cluster_dict[i] = utterances.values.tolist()[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 27.59 seconds \n",
      "\n",
      "Utterances: 69 \n",
      "\n",
      "Final Overall WCAS: 0.3878871770983733 \n",
      "\n",
      "Cluster \t WCAS \t \t \t \t Length\n",
      "0 \t \t 0.5924132844543113 \t \t 4\n",
      "1 \t \t 0.32823858140536927 \t \t 4\n",
      "2 \t \t 0.38888888888888884 \t \t 3\n",
      "5 \t \t 0.3960108206551019 \t \t 4\n",
      "7 \t \t 0.25796287669169626 \t \t 4\n",
      "15 \t \t 0.39814239699997195 \t \t 4\n",
      "19 \t \t 0.35355339059327373 \t \t 2\n",
      "\n",
      "\n",
      "Cluster \t Utterance\n",
      "0 \t \t [\"I'm not happy with the service, what do I have to do to submit a consumer complaint?\", 'I am not happy with the service, could I file a consumer complaint?', 'I am discontent with the service, how could I file a fucking complaint?', 'im discontent with the service and i want to file a consumer complaint']\n",
      "1 \t \t ['do you mind asking Alexa how I could check my order, please?', 'please, can you ask Alexa where I could notify problems making a payment?', 'I have an issue making a payment, how can I report it?', 'I have an issue when trying to make a payment with card, can you help me?']\n",
      "2 \t \t ['ask alexa how to call customer supoort, please', 'ask alexa how i could remove my account, please', 'can u ask alexa where to see the cancellation penalty?']\n",
      "5 \t \t ['could i create a user account?', 'i wanna know if i could have more than one user account', 'i dont have an online account, what do i have to do to open one?', 'tell me if I can create more than one user account with the same email address']\n",
      "7 \t \t ['I get an error message when I attempt to pay with card', 'an error pops when I try to pay, can you help me?', 'I purchased an item, help me change my order', \"I didn't receive my invoice, view it\"]\n",
      "13 \t \t ['i have a question, what do i have to do to call customer support?']\n",
      "15 \t \t ['can you give me information about the email of the Client Service?', 'can you give me information about the email of the Customer Support?', 'can you show me the email of Customer Service?', 'can you give me information about the delivery period?']\n",
      "19 \t \t ['can you obtain a bill?', 'i wanna obtain a reimbursement, what should i do?']\n",
      "26 \t \t ['i want to ask for information about geting a bill']\n",
      "32 \t \t ['is it possible to track my orders?']\n",
      "42 \t \t ['help me check my deliveties']\n",
      "44 \t \t ['can i register several fucking online accounts with a single email ?']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "start_time = time.time()\n",
    "cluster_dict, overall_wcas_list, overall_wcas, wcas = utterance_hierarchical_clustering(cluster_dict, 0.2, stop_words)\n",
    "print('Time taken: {:.2f} seconds \\n'.format(time.time()-start_time))\n",
    "print('Utterances: {} \\n'.format(len(utterances['Utterance'])))\n",
    "print_results(cluster_dict, overall_wcas, wcas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clusters do not seem distinct. For example, cluster 19 has 2 completely different sentences.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
