{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing and manipulating data\n",
    "import pandas as pd\n",
    "\n",
    "# For text preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Train word embedding model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Splitting dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Clustering\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Process Text Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "    \"\"\"\n",
    "    Takes sentence (string), removed special characters, removes stop words, tokenize by words, and converts to lowercase.\n",
    "    Returns list of strings that are the cleaned words from the sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove special characters\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # Convert to lower case and split words into separate strings\n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # Remove stop words\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Retain cleaned lists of words from sentence\n",
    "    cleaned_words = list(set([w for w in words if w not in stopword_set]))\n",
    "\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bitext_free_dataset.csv', header = 0)\n",
    "\n",
    "# List of tokenized sentences with stopwords removed\n",
    "sentences = [preprocess(x[0]) for x in data.values.tolist()]\n",
    "\n",
    "# List of raw sentences (not pre-processed)\n",
    "not_pp_sentences = [x[0] for x in data.values.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word Embedding Model\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Sentences to Vectors with Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "sentences_as_vectors = []\n",
    "for sentence in not_pp_sentences:\n",
    "    \n",
    "    # Mean of word vectors from Word2Vec model become the sentence vector\n",
    "    sentences_as_vectors.append(np.mean([model[word] for word in preprocess(sentence)],axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AgglomerativeClustering from SciKit-Learn recursively merges the pair of clusters that minimally increases a given linkage distance (1).  \n",
    " - **n_clusters:** Number of clusters to stop at. I chose 27 since the dataset I am comparing against has 27 intents that were identified.  \n",
    " - **affinity:**  Since magnitude can be considered as I am passing the algorithm vectors which represent means of vectors for the words in each sentence, I use euclidean distance over cosine distance (2).  \n",
    " - **linkage:**  Which linkage criterion to use to merge pairs of clusters. Average means it will merge the two clusters with the smallest average distance between each sentence of the two clusters.  \n",
    "\n",
    "\n",
    "References:  \n",
    " 1. https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    " 2. https://cmry.github.io/notes/euclidean-v-cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
       "                        connectivity=None, distance_threshold=None,\n",
       "                        linkage='average', memory=None, n_clusters=27)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster_model = AgglomerativeClustering(n_clusters=27, affinity='euclidean', linkage='average')\n",
    "cluster_model.fit(sentences_as_vectors)\n",
    "cluster_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_model.n_clusters_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Store Labeled Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Utterances by Cluster:\n",
      "\n",
      "19    3725\n",
      "0     2613\n",
      "7     2068\n",
      "12    1640\n",
      "2     1602\n",
      "20    1525\n",
      "8     1272\n",
      "4     1071\n",
      "10    1025\n",
      "9      989\n",
      "6      750\n",
      "3      648\n",
      "23     554\n",
      "17     503\n",
      "5      330\n",
      "13     287\n",
      "1      252\n",
      "14     222\n",
      "15     202\n",
      "26     146\n",
      "21     120\n",
      "11     103\n",
      "18      15\n",
      "22       5\n",
      "16       3\n",
      "24       1\n",
      "25       1\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'Utterances': not_pp_sentences, 'Label': cluster_model.labels_}) # Labels are accessed from fit model\n",
    "print('Number of Utterances by Cluster:\\n')\n",
    "print(data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Lex allows a maximum of 1500 utterances per intent. Since the idea behind spliting testing and training sets is to maximize our training data while still allowing for adequate testing, I maximize my training data by splitting 60% test, 40% train. This gives me just under the max amount of training data for my largest cluster. Clusters with 5 or less utterances in the training or test set will be withheld as \"fallback\" or unhandled utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.6)\n",
    "train.to_excel('train.xlsx')\n",
    "test.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Cluster Counts\n",
      "\n",
      "19    1490\n",
      "0      999\n",
      "7      824\n",
      "12     653\n",
      "2      644\n",
      "20     599\n",
      "8      515\n",
      "4      430\n",
      "10     408\n",
      "9      400\n",
      "6      294\n",
      "3      273\n",
      "23     230\n",
      "17     194\n",
      "5      131\n",
      "13     116\n",
      "1      112\n",
      "15      89\n",
      "14      89\n",
      "26      65\n",
      "21      53\n",
      "11      48\n",
      "18       7\n",
      "22       4\n",
      "16       1\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "\n",
      "Test Data Cluster Counts\n",
      "\n",
      "19    2235\n",
      "0     1614\n",
      "7     1244\n",
      "12     987\n",
      "2      958\n",
      "20     926\n",
      "8      757\n",
      "4      641\n",
      "10     617\n",
      "9      589\n",
      "6      456\n",
      "3      375\n",
      "23     324\n",
      "17     309\n",
      "5      199\n",
      "13     171\n",
      "1      140\n",
      "14     133\n",
      "15     113\n",
      "26      81\n",
      "21      67\n",
      "11      55\n",
      "18       8\n",
      "16       2\n",
      "25       1\n",
      "24       1\n",
      "22       1\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Training Data Cluster Counts\\n')\n",
    "print(train['Label'].value_counts())\n",
    "print('\\n')\n",
    "print('Test Data Cluster Counts\\n')\n",
    "print(test['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
